{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceafe4d3",
   "metadata": {},
   "source": [
    "# üß† Notebook 1.0 ‚Äî Introduction to RAG (local with Ollama)\n",
    "\n",
    "Welcome to the first module of the **AI Agents with RAG** course!\n",
    "\n",
    "**Objectives**\n",
    "- ‚úÖ What a RAG system is\n",
    "- ‚úÖ Difference between a ‚Äúvanilla‚Äù LLM and RAG\n",
    "- ‚úÖ Build your first **local** RAG with **LangChain + Ollama + Chroma**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993416f",
   "metadata": {},
   "source": [
    "## 0.0 üß© Prerequisites ‚Äî Install Ollama **(from Terminal/PowerShell)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f2065b",
   "metadata": {},
   "source": [
    "Run **only once**, before opening the notebook\n",
    "\n",
    "- From Terminal/PowerShell\n",
    "- Inside your working environment\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ûú **MacOS (Homebrew)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e7b0b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.5.7\n"
     ]
    }
   ],
   "source": [
    "# bash\n",
    "#!brew install --cask ollama\n",
    "!ollama --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fc7ca5",
   "metadata": {},
   "source": [
    "### ‚ûú **Windows (PowerShell)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49655107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bash\n",
    "#!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Start the service in the current session:\n",
    "#!ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87591b",
   "metadata": {},
   "source": [
    "If the model loads correctly, Ollama is working üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7630f6e9",
   "metadata": {},
   "source": [
    "## 1.0 üìö What is RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518c28d",
   "metadata": {},
   "source": [
    "**RAG = Retrieval Augmented Generation**\n",
    "\n",
    "It is a technique that allows an LLM to **respond based on external documents**, instead of relying solely on its internal training.\n",
    "\n",
    "Useful when:\n",
    "- you want *accurate* answers based on your documents\n",
    "- you want to avoid hallucinations\n",
    "- you need updated information\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c55a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# powershell / terminal\n",
    "#$env:Path += \";C:\\Users\\<TUO_UTENTE>\\AppData\\Local\\Programs\\Ollama\"\n",
    "#ollama --version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374785c0",
   "metadata": {},
   "source": [
    "A standard LLM **does not know your documents**, unless they were in its training set.\n",
    "\n",
    "Therefore:\n",
    "- it invents citations\n",
    "- it hallucinates dates\n",
    "- it produces generic answers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11816efa",
   "metadata": {},
   "source": [
    "### üîç What RAG adds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a21612",
   "metadata": {},
   "source": [
    "**RAG pipeline:**\n",
    "1. Split documents into chunks\n",
    "2. Convert chunks into embeddings\n",
    "3. Save in a vector database\n",
    "4. When you ask a question ‚Üí retrieve the most relevant chunks\n",
    "5. Pass question + retrieved chunks to the LLM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fb82901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: accepts 1 arg(s), received 8\n",
      "\u001b[?25lpulling manifest √¢¬†‚Äπ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†‚Ñ¢ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†¬π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†¬∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†¬º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†¬¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†¬¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†¬ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†‚Ä° \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†ÔøΩ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†‚Äπ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†‚Ñ¢ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†¬π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†¬∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†¬º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†¬¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†¬¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest √¢¬†¬ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% √¢‚Äì‚Ä¢√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÔøΩ 274 MB                         \n",
      "pulling c71d239df917... 100% √¢‚Äì‚Ä¢√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÔøΩ  11 KB                         \n",
      "pulling ce4a164fc046... 100% √¢‚Äì‚Ä¢√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÔøΩ   17 B                         \n",
      "pulling 31df23ea7daa... 100% √¢‚Äì‚Ä¢√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÀÜ√¢‚ÄìÔøΩ  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2:3b     # lighter, recommended for less powerful laptops\n",
    "#ollama pull llama3.1:8b     # higher quality, requires more powerful machines\n",
    "\n",
    "# Embedding model for retrieval\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18786de3",
   "metadata": {},
   "source": [
    "### 2.1 üìÅ Load documents (txt / pdf / web page‚Ä¶)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdcc8bc",
   "metadata": {},
   "source": [
    "Load any text file you want to query.\n",
    "\n",
    "Below: we load a sample file from disk.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58334e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install langchain langchain-community langchain-text-splitters chromadb ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13005b6",
   "metadata": {},
   "source": [
    "Choose chunk size and overlap so the model can read coherent portions.\n",
    "\n",
    "Typical values:\n",
    "- **chunk_size = 500‚Äì1500** characters\n",
    "- **chunk_overlap = 50‚Äì150**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ee86a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wudyt\\AppData\\Local\\Temp\\ipykernel_16252\\3705423524.py:3: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3.2:3b\", temperature=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok, I'm ready!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", temperature=0)\n",
    "print(llm.invoke(\"Just say: ok, I'm ready!\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1a439",
   "metadata": {},
   "source": [
    "This creates the vector database where all embeddings will be stored.\n",
    "\n",
    "If the notebook is restarted, Chroma will persist data to the folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224cd785",
   "metadata": {},
   "source": [
    "## 2.4 üîç Build the retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57064f11",
   "metadata": {},
   "source": [
    "Retrieves the **k most relevant chunks**.\n",
    "\n",
    "Typical values: `k = 3` or `4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be8bcb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our \"external source of information\" (it could be text from pdfs, word docs, etc..)\n",
    "docs = [\n",
    "    \"TechCorp is a company that develops software for the healthcare sector.\",\n",
    "    \"TechCorp's sales department uses Salesforce as its CRM.\",\n",
    "    \"The cloud infrastructure is based on AWS.\",\n",
    "    \"TechCorp offers 24/7 technical support for enterprise clients.\",\n",
    "    \"The vacation policy provides 25 days per year for each employee.\",\n",
    "    \"The mobile application is developed in Flutter and updated monthly.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7e72d",
   "metadata": {},
   "source": [
    "Here we combine:\n",
    "- the retriever (which extracts the right documents)\n",
    "- the LLM (which generates the final answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "416cc327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla LLM: I don't have specific information about the policies of TechCorp, as I'm a large language model, I don't have access to that kind of data. However, I can provide some general information about typical vacation and leave policies in the tech industry.\n",
      "\n",
      "In many companies, including tech corporations, employees are typically entitled to a certain number of paid vacation days, sick leave, and personal days per year. The exact number of days off can vary widely depending on factors such as job title, location, and company size.\n",
      "\n",
      "Some common policies include:\n",
      "\n",
      "* Annual vacation time: 10-20 days\n",
      "* Sick leave: 5-10 days\n",
      "* Personal days: 5-10 days\n",
      "* Holidays: 8-12 paid holidays per year\n",
      "\n",
      "It's worth noting that these are just general estimates, and actual policies can vary significantly from company to company. If you're interested in knowing the specific policies of TechCorp, I would recommend checking your employee handbook or speaking with HR directly.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Pick the model previously pulled (es. llama3.2:3b o llama3.1:8b)\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", temperature=0)\n",
    "\n",
    "question = \"How many days off can one take at TechCorp?\"\n",
    "response = llm.invoke(question).content\n",
    "print(\"Vanilla LLM:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0104d",
   "metadata": {},
   "source": [
    "> *`AttributeError: 'ChatOllama' object has no attribute 'predict'`*\n",
    "\n",
    "- With modern APIs, use `.invoke()` (as in the examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d35c5838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wudyt\\AppData\\Local\\Temp\\ipykernel_16252\\3944491745.py:19: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 1) Text splitting\n",
    "documents = [\n",
    "    Document(page_content=text, metadata={\"source\": f\"Doc_{i+1}\"})\n",
    "    for i, text in enumerate(docs)\n",
    "]\n",
    "\n",
    "# Splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "splitted = splitter.split_documents(documents)\n",
    "\n",
    "# 2) Embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# 3) Vector store\n",
    "vectorstore = Chroma.from_documents(splitted, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 4) RetrievalQA\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"You are an assistant that answers ONLY using the information in the DOCUMENTS.\\n\"\n",
    "        \"If the answer is not present, clearly say: 'Not present in the documents.'\\n\\n\"\n",
    "        \"DOCUMENTS:\\n{context}\\n\\n\"\n",
    "        \"Question: {question}\\n\"\n",
    "        \"Concise answer in English:\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,                           # definito nella Sezione 4 (ChatOllama)\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": qa_prompt},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc261329",
   "metadata": {},
   "source": [
    "## 6. ‚öñÔ∏è Compare: LLM vs RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9165b7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: How many vacation days does TechCorp's policy provide?\n",
      "‚Üí LLM Vanilla: I don't have information about TechCorp's specific vacation day policy. If you're looking for details on a company's benefits or policies, I recommend checking their official website or HR department for the most accurate and up-to-date information.\n",
      "‚Üí RAG: 25 days.\n",
      "   [1] Doc_5 | The vacation policy provides 25 days per year for each employee....\n",
      "   [2] Doc_4 | TechCorp offers 24/7 technical support for enterprise clients....\n",
      "   [3] Doc_1 | TechCorp is a company that develops software for the healthcare sector....\n",
      "\n",
      "Question: What is the language used for the mobile app?\n",
      "‚Üí LLM Vanilla: I can't determine which specific mobile app you are referring to. There are many different apps with various programming languages used in their development. If you could provide more information about the app, such as its name or category, I may be able to help you better.\n",
      "‚Üí RAG: Flutter.\n",
      "   [1] Doc_6 | The mobile application is developed in Flutter and updated monthly....\n",
      "   [2] Doc_2 | TechCorp's sales department uses Salesforce as its CRM....\n",
      "   [3] Doc_1 | TechCorp is a company that develops software for the healthcare sector....\n",
      "\n",
      "Question: Who provides the cloud infrastructure?\n",
      "‚Üí LLM Vanilla: There are several companies that provide cloud infrastructure, including:\n",
      "\n",
      "1. Amazon Web Services (AWS): AWS is one of the largest and most popular cloud providers, offering a wide range of services such as computing power, storage, databases, analytics, machine learning, and more.\n",
      "2. Microsoft Azure: Azure is Microsoft's cloud platform, providing a range of services including computing power, storage, networking, and AI capabilities.\n",
      "3. Google Cloud Platform (GCP): GCP is Google's cloud platform, offering a range of services such as computing power, storage, databases, machine learning, and more.\n",
      "4. IBM Cloud: IBM Cloud provides a range of services including computing power, storage, networking, and AI capabilities, with a focus on enterprise customers.\n",
      "5. Oracle Cloud Infrastructure (OCI): OCI is Oracle's cloud platform, offering a range of services such as computing power, storage, databases, and machine learning.\n",
      "6. Alibaba Cloud: Alibaba Cloud is the cloud platform of Alibaba Group, providing a range of services including computing power, storage, networking, and AI capabilities.\n",
      "7. Rackspace: Rackspace is a managed cloud provider that offers a range of services including computing power, storage, networking, and IT management.\n",
      "8. VMware Cloud: VMware Cloud is a hybrid cloud platform that allows customers to run their applications in the cloud or on-premises.\n",
      "\n",
      "These are just a few examples of companies that provide cloud infrastructure. There are many other providers available, each with their own strengths and weaknesses.\n",
      "\n",
      "It's worth noting that some companies also offer their own private clouds, which are managed by the company itself and not provided through a third-party provider.\n",
      "‚Üí RAG: AWS.\n",
      "   [1] Doc_3 | The cloud infrastructure is based on AWS....\n",
      "   [2] Doc_4 | TechCorp offers 24/7 technical support for enterprise clients....\n",
      "   [3] Doc_1 | TechCorp is a company that develops software for the healthcare sector....\n",
      "\n",
      "Question: What tool does the sales department use?\n",
      "‚Üí LLM Vanilla: I don't have enough information to determine which specific sales department you are referring to. There are many different types of sales departments across various industries, and each may use a unique set of tools.\n",
      "\n",
      "However, some common tools used by sales departments include:\n",
      "\n",
      "1. CRM (Customer Relationship Management) software: This is a type of software that helps sales teams manage their interactions with customers, track leads, and analyze sales performance.\n",
      "2. Sales automation tools: These are tools that automate repetitive tasks, such as data entry or email follow-ups, to help sales teams focus on more strategic activities.\n",
      "3. Sales analytics tools: These are tools that provide insights into sales performance, such as sales forecasting, pipeline analysis, and customer behavior.\n",
      "4. Communication and collaboration tools: These are tools that enable sales teams to communicate and collaborate with each other, such as email, phone, or video conferencing software.\n",
      "\n",
      "If you could provide more context or information about the specific sales department you are referring to, I may be able to provide a more specific answer.\n",
      "‚Üí RAG: Salesforce.\n",
      "   [1] Doc_2 | TechCorp's sales department uses Salesforce as its CRM....\n",
      "   [2] Doc_4 | TechCorp offers 24/7 technical support for enterprise clients....\n",
      "   [3] Doc_1 | TechCorp is a company that develops software for the healthcare sector....\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"How many vacation days does TechCorp's policy provide?\",\n",
    "    \"What is the language used for the mobile app?\",\n",
    "    \"Who provides the cloud infrastructure?\",\n",
    "    \"What tool does the sales department use?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    # LLM Vanilla (senza documenti)\n",
    "    print(\"‚Üí LLM Vanilla:\", llm.invoke(q).content)\n",
    "\n",
    "    # RAG (retrieval + generazione)\n",
    "    out = qa_chain.invoke({\"query\": q})\n",
    "    print(\"‚Üí RAG:\", out[\"result\"])\n",
    "\n",
    "    # Mostra le fonti trovate dal retriever\n",
    "    for i, s in enumerate(out.get(\"source_documents\", []), 1):\n",
    "        print(f\"   [{i}] {s.metadata.get('source','?')} | {s.page_content[:80]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708a0d9c",
   "metadata": {},
   "source": [
    "## 7. üß™ Exercises\n",
    "\n",
    "Here's the English translation:\n",
    "-Add a new document to the knowledge base and ask a question again.\n",
    "-Change the chunking parameters and evaluate the impact on the responses.\n",
    "-Try using another OpenAI model with different temperature settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e576cdc",
   "metadata": {},
   "source": [
    "## 8. ‚úÖConclusions & Next Steps\n",
    "\n",
    "You've built your first RAG system! üéâ\n",
    "\n",
    "‚úÖ Understood the difference between Vanilla LLM and RAG\n",
    "‚úÖ Implemented a local RAG with Ollama (LLM + embeddings) and Chroma\n",
    "‚úÖ Tested real questions on company data and cited the sources of the chunks\n",
    "\n",
    "üìå Next step: build your own richer company dataset and implement source citation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e47b0f1",
   "metadata": {},
   "source": [
    "## 9) üß∞ Troubleshooting\n",
    "\n",
    ">**Windows: `ollama` not recognized in the notebook**\n",
    "\n",
    "Make sure it works in PowerShell; if needed, temporarily add it to PATH (see \"Prerequisites\" section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e10e5f9",
   "metadata": {},
   "source": [
    ">**Ollama connection error** (*connection refused* / *Failed to connect*)\n",
    "\n",
    "- Start the service and verify:\n",
    "    - macOS/Win: `ollama --version`, then `ollama list`\n",
    "    - Linux: `ollama serve` (if not already running)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38705ab8",
   "metadata": {},
   "source": [
    ">**model not found**  \n",
    "- Run `ollama pull llama3.2:3b` (LLM) and `ollama pull nomic-embed-text` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e6326",
   "metadata": {},
   "source": [
    ">**ImportError** (`langchain_text_splitters` o similar)  \n",
    "- Re-install pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73efad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-community langchain-text-splitters chromadb ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be35eca",
   "metadata": {},
   "source": [
    ">*`AttributeError: 'ChatOllama' object has no attribute 'predict'`*\n",
    "\n",
    "- Current API uses `.invoke()` (as used in the examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cb2bcf",
   "metadata": {},
   "source": [
    ">**RAG responds in a generic/incorrect way**  \n",
    "- 1. Decrease/increase `k` (e.g. 3‚Üí4) \n",
    "- 2. Review `chunk_size/overlap` \n",
    "- 3. Print the retrieved documents to understand what it's reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c91a908-d8d1-422e-bb1f-6285c3ee90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"How many days off can be taken at TechCorp?\"\n",
    "for i, d in enumerate(retriever.get_relevant_documents(q), 1):\n",
    "    print(f\"[{i}] {d.metadata['source']} | {d.page_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_course",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
